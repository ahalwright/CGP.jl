Write down entropy and information formulas so I remember them.
References:
Christof Adami 2015 "What is information"
Thomas Cover & Joy Thomas "Elements of information theory" (1991)

Equation numbers with "3" are from Adami, with "2" are from Cover.

"The entropy of a random variable is a measure of the amount of information required on the average
to describe the random variable."  (Section 2.3)

Entropy of a random variable $X$:  $H(X) = - \sum_i p_i \log p_i$      (3.1)

Maximum entry of a random variable with $N$ states is $H_{max} = \log N$.    (3.2)

Joint entropy:

$H(X,Y) = - \sum_x \sum_y p(x,y) \log p(x,y)$

Conditional entropy:

Let $Y$ be a random variable with two values $Y=0$ and $Y=1$ with probabilities $q_0$ and $q_1$ respectively.

$H(X|Y) = q_0 H(X|Y=0) + q_1 H(X|Y=1)$     (3.6)

In general, $H(X|Y) = \sum_y p(y) H(X|Y=y) = - \sum_y \sum_x p(x,y) \log p(x|y) = \sum_y p(y) H(X|Y=y) (2.12)

Chain rule:  $H(X,Y) = H(X) + H(Y|X)$   (2.14)

Relative entropy (Kullback Leibler distance)  Not a true distance: not symmetric, no triangle inequality.

$D( p \| q ) = \sum_x p(x) \log \frac{p(x)}{q(x)} = E_p \log( p(X)/q(X) )$    (2.27)

Mutual information (shared entropy):

$I( X; Y ) = \sum_x \sum_y p(x,y)/p(x)/p(y) = D( p(x,y) \| p(x)p(y) ) = E_{p(x,y)} log p(X,Y)/p(X)/p(Y)$   (2.30)

What $X$ knows about $Y$ == what $Y$ knows about $X$.  Adami page 11.

$I(X; Y) = H(X) - H(X|Y) = H(Y) - H(Y|X)$    (2.40)

Differences between entropies (for example, before and after a measurement) are called information. 
Information, you see, is real. Entropy on the other hand, is in the eye of the beholder
