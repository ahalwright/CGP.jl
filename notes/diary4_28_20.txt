4/28/20

Exploring the difference between mutual_information(P,Q) and mutual_information([P,Q])
where P1 and P2 are MIPopulations.

mutual_information(P,Q) calls pops_to_dist(P,Q) which is a distribution over ordered pairs
(p,q) from P and Q.  So entropy is reduced from the maximum only when there are multiple
identical pairs (p,q).  (As of 4/28, only works on lycosa version of entropy.)

mutual_information([P,Q]) calls pops_to_tbl([P,Q]), and then sherwin_mutual_information
on the resulting table.

Examples:
P  = [0x7,0x4,0x7,0x3]
Q  = [0x3,0x3,0x3,0x1]
PP = [0x7,0x4,0x2,0x7]

julia> pops_to_dist(P,Q)
Dict{Any,Any} with 3 entries:
  (0x04, 0x03) => 0.25
  (0x03, 0x01) => 0.25
  (0x07, 0x03) => 0.5
julia> mutual_information(P,Q)
0.8112781244591329
julia> pops_to_dist(PP,Q)
Dict{Any,Any} with 4 entries:
  (0x04, 0x03) => 0.25
  (0x02, 0x03) => 0.25
  (0x07, 0x03) => 0.25
  (0x07, 0x01) => 0.25
julia> mutual_information(PP,Q)
0.31127812445913294

julia> ttble = pops_to_tbl([P,Q])
2×4 Array{Float64,2}:
 0.25  0.125  0.125  0.0
 0.0   0.0    0.375  0.125
julia> sherwin_mutual_information(ttbl)
0.5943609377704335
julia> mutual_information([P,Q])
0.5943609377704335
julia> pops_to_tbl([PP,Q])
2×5 Array{Float64,2}:
 0.25  0.125  0.125  0.0    0.0
 0.0   0.0    0.0    0.375  0.125
julia> sherwin_mutual_information(pops_to_tbl([PP,Q]))
0.9999999999999998
julia> mutual_information([PP,Q])
0.9999999999999998

