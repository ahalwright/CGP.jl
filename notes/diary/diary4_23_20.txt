4/23/20:
I think that the better way to compute the Tononi degeneracy, complexity, etc. of a collection
of gates is by taking the output of each gate as a bit string over the 2^numinputs possible inputs
to the circuit, and then by combining this collection of bit strings into a vector of probabilities
of a 1 in the corresponding bit positions.

For examples, suppose numinputs==2, and the gate collection has 3 gates whose outputs are given as 
bit strings [0xe, 0x5, 0xa] = [1110, 0101, 1010].   So there are 4 bit positions which I am
numbering from right to left (least significant to most significant), and their probabilities of
1 bits are [1/3, 2/3, 2/3, 2/3].  The probabilities are computed by the get_probs() function of
InfTheory.jl.

So I changed every call to get_bits() to a call to get_probs() in InfTheory.jl.
The previous version is in stash/InfTheory4_23_20.jl.

However, this is not the way the Macia and Sole compute these quantities.
The examples in test/macia_examples.jl show that my computations that use get_bits() agree
with Macia's method of computing mutual information.

4/24/20:
Tried to compute mutual information using the get_probs() approach, and it didn't work because
joint_entropy is not defined.  joint_entropy() requires a joint probability distribution.

However, sherwin_mutual_information() works.  Example:
julia> pr1 = [1/4, 1/2, 0.0, 1/4]
julia> pr2 = [1/8, 1/4, 3/8, 1/4]
julia> tbl = pops_to_tbl([pr1,pr2]) 
julia> sherwin_mutual_information(tbl)
0.23345859334434982     # Compare with different results in diary4_25_20.txt

However, 
julia> sherwin_mutual_information([pr1,pr2])
doesn't work because of a type error.  

Idea that doesn't work:  continuous entropy because it is based on float elements of populations
rather than probabilities produced by get_probs()
